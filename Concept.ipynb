{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e908148-724a-408e-bab5-d8a2ba4329e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwalke/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import IMDB, DBLP\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import normalized_mutual_info_score, f1_score, accuracy_score\n",
    "from HeteroNestedCV import NestedTransductiveCV\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import copy\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54bde86-724e-445e-8e8b-1b78aef60ee4",
   "metadata": {},
   "source": [
    "## Future improvemtn ideas\n",
    "- Representationen ähnlicher Entitäten des gleichen Knotentyps mit einander verknüpfen als zusatz-info nach dem über meta-paths informationen aggregiert wurden\n",
    " -> Neue Pseudo-attention (e.g.,Sylvester Stalone ist ähnlich zu Arnold Schwarzenegger - Action basiert)\n",
    "- Expansion not on semantic level and instead only expand vector to maxiumum vector size to reduce #trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48098732-d243-414e-b044-787482487ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = IMDB(root= \"./data/IMDB\")\n",
    "dblp = DBLP(root= \"./data/DBLP\")\n",
    "data = dblp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039638f9-586c-4b09-ae0d-5be3afbd70f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 3,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"author\"].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43a8b67-788c-4539-8223-12f0e104fab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  author={\n",
       "    x=[4057, 334],\n",
       "    y=[4057],\n",
       "    train_mask=[4057],\n",
       "    val_mask=[4057],\n",
       "    test_mask=[4057],\n",
       "  },\n",
       "  paper={ x=[14328, 4231] },\n",
       "  term={ x=[7723, 50] },\n",
       "  conference={ num_nodes=20 },\n",
       "  (author, to, paper)={ edge_index=[2, 19645] },\n",
       "  (paper, to, author)={ edge_index=[2, 19645] },\n",
       "  (paper, to, term)={ edge_index=[2, 85810] },\n",
       "  (paper, to, conference)={ edge_index=[2, 14328] },\n",
       "  (term, to, paper)={ edge_index=[2, 85810] },\n",
       "  (conference, to, paper)={ edge_index=[2, 14328] }\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6420b-b965-4f07-a538-c1b34a7b7b67",
   "metadata": {},
   "source": [
    "## Add features (categorical enc. for now) to nodes withot features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab735dcf-1a66-4bbc-8295-9e5baa817a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 5],\n",
       "        [ 6],\n",
       "        [ 7],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [11],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [17],\n",
       "        [18],\n",
       "        [19]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"conference\"].x = torch.arange(data[\"conference\"].num_nodes).unsqueeze(-1)\n",
    "data[\"conference\"].x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d4fa9-db38-49f7-97f9-dd27f856e458",
   "metadata": {},
   "source": [
    "## Node type feature padding for preserving semantic differences bewteen individual node tyypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04a28a9-0de1-42c0-8b62-6bdbc006e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pad\n",
    "\n",
    "node_types, edge_types = data.metadata()\n",
    "node_type_shapes = torch.tensor([data[node_type].x.shape[-1] for node_type in data.metadata()[0]])\n",
    "\n",
    "ident = torch.ones(len(node_types), len(node_types))\n",
    "right_pad_mat = torch.triu(ident) - torch.diag(torch.ones(len(node_types)), 0)\n",
    "left_pad_mat = right_pad_mat.t()\n",
    "left_pad_mat.type(torch.long) * node_type_shapes\n",
    "pad_matrix = torch.cat([left_pad_mat.type(torch.long) @node_type_shapes.unsqueeze(1), right_pad_mat.type(torch.long) @node_type_shapes.unsqueeze(1)], dim = -1) \n",
    "\n",
    "for i, node_type in enumerate(node_types):\n",
    "    data[node_type].x = pad(data[node_type].x, pad_matrix[i].tolist(), \"constant\", 0).type(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf0ef5-63de-44f7-a280-434fc9a9ea67",
   "metadata": {},
   "source": [
    "## Input data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9c0faa-ae5b-408f-aba6-ff188db4344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGraphData:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0ac00-6003-45f5-822b-24ef945b5eaa",
   "metadata": {},
   "source": [
    "## Graph for graph schema with DFS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0fac4e1-eb48-47e5-84c7-12156b6a045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge:\n",
    "    def __init__(self, source_node, edge_type, target_node):\n",
    "        self.source_node = source_node\n",
    "        self.edge_type:str = edge_type\n",
    "        self.target_node = target_node\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"\n",
    "        {self.source_node}->{self.edge_type}->{self.target_node}\n",
    "        \"\"\"\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.targets = []\n",
    "        self.sources = []\n",
    "\n",
    "    def add_target_edge(self, target_edge):\n",
    "        self.targets.append(target_edge)\n",
    "\n",
    "    def add_source_edge(self, source_edge):\n",
    "        self.sources.append(source_edge)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"{self.name}()\"\"\"\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "        self.nodes_index = dict()\n",
    "        self.edges = []\n",
    "        self.dfs_paths = []\n",
    "\n",
    "    def add_node(self,node_name):\n",
    "        node = Node(node_name)\n",
    "        self.nodes_index[node_name] = node\n",
    "        self.nodes.append(node)\n",
    "\n",
    "    def add_edge(self, source_node_name, edge_type, target_node_name):\n",
    "        if source_node_name not in self.nodes_index:\n",
    "            self.add_node(source_node_name)\n",
    "        if target_node_name not in self.nodes_index:\n",
    "            self.add_node(target_node_name)\n",
    "        source_node = self.nodes_index[source_node_name]\n",
    "        target_node = self.nodes_index[target_node_name]\n",
    "        \n",
    "        edge = Edge(source_node, edge_type, target_node)\n",
    "        self.edges.append(edge)\n",
    "        source_node.add_target_edge(edge)\n",
    "        target_node.add_source_edge(edge)\n",
    "\n",
    "    def dfs_per_node(self, start_node_name, max_depth):\n",
    "        assert start_node_name in self.nodes_index, f\"Node {start_node_name} is not in the Graph ({self})\"\n",
    "        self.dfs_paths = []\n",
    "        start_node = self.nodes_index[start_node_name]\n",
    "        dfs_path = []\n",
    "        \n",
    "        def traverse(node, curr_depth, dfs_path):\n",
    "            if node is None or len(dfs_path) == max_depth: \n",
    "                self.dfs_paths.append(dfs_path)\n",
    "                dfs_path = []\n",
    "                return\n",
    "            for edge in node.targets:\n",
    "                \n",
    "                traverse(edge.target_node, curr_depth + 1, [*dfs_path, edge])\n",
    "        \n",
    "        traverse(start_node, 0, dfs_path) \n",
    "        return self.dfs_paths\n",
    "\n",
    "    def dfs_per_node_include_all_paths(self, start_node_name, max_depth):\n",
    "        all_meta_paths_per_node = []\n",
    "        for max_depth_i in range(1, max_depth + 1):\n",
    "            all_meta_paths_per_node.extend(self.dfs_per_node(start_node_name, max_depth_i))\n",
    "        self.dfs_paths = all_meta_paths_per_node\n",
    "        return self.dfs_paths\n",
    "\n",
    "    def dfs(self, max_depth):\n",
    "        all_meta_paths = []\n",
    "        for node in self.nodes:\n",
    "            all_meta_paths.extend(self.dfs_per_node(node.name, max_depth))\n",
    "        self.dfs_paths = all_meta_paths\n",
    "        return self.dfs_paths\n",
    "\n",
    "    def dfs_include_all_paths(self, max_depth):\n",
    "        all_meta_paths = []\n",
    "        for node in self.nodes:\n",
    "            all_meta_paths.extend(self.dfs_per_node_include_all_paths(node.name, max_depth))\n",
    "        self.dfs_paths = all_meta_paths\n",
    "        return self.dfs_paths\n",
    "\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"\n",
    "        self.nodes: {self.nodes},\n",
    "        self.edges: {self.edges},\n",
    "        self.dfs_paths: {self.dfs_paths},\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8113635-2871-4f39-92a8-152f4732f51a",
   "metadata": {},
   "source": [
    "## Metapath creation or automatic construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a7233c-4ada-4ead-877e-de3ce9ff76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaPaths:\n",
    "    def __init__(self, target_node_type):\n",
    "        self.meta_path_dict = {}\n",
    "        self.target_node_type = target_node_type\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def graph_meta_path_to_triple_meta_path(meta_path):\n",
    "        meta_path_triples = []\n",
    "        for meta_path_part in meta_path:\n",
    "            meta_path_triples.append((\n",
    "                meta_path_part.source_node.name,\n",
    "                meta_path_part.edge_type,\n",
    "                meta_path_part.target_node.name,\n",
    "            ))\n",
    "        return meta_path_triples\n",
    "\n",
    "    def construct_meta_paths(self, metadata, max_depth):\n",
    "        assert max_depth >= 1, \"max_depth needs to be greater or equal to one to construct meta_paths since a distance of less than one does not consider the graph.\"\n",
    "        _, edge_types = metadata\n",
    "        graph_schema = Graph()\n",
    "        for edge_type in edge_types:\n",
    "            graph_schema.add_edge(*edge_type)\n",
    "        for max_depth_i in range(1, max_depth + 1):\n",
    "            meta_paths = graph_schema.dfs(max_depth_i)\n",
    "            \n",
    "            meta_paths_for_target_node = list(filter(lambda meta_path: meta_path[-1].target_node.name == self.target_node_type, meta_paths))     \n",
    "            meta_paths_for_target_node_triples = list(map(lambda meta_path: MetaPaths.graph_meta_path_to_triple_meta_path(meta_path), meta_paths_for_target_node))\n",
    "            self.add_meta_paths(max_depth_i, meta_paths_for_target_node_triples)\n",
    "        return self.meta_path_dict\n",
    "\n",
    "    \n",
    "    def assert_meta_path(self, hop, meta_path):\n",
    "        assert len(meta_path) == hop, f\"Metapath with length {len(meta_path)} should have the same length as the number of hops in the graphs: {hop}.\"\n",
    "        valid_len_meta_path_parts = [len(meta_path_part) == 3 for meta_path_part in meta_path]\n",
    "        assert all(valid_len_meta_path_parts), f\"All meta path parts should have length 3 (source_type, edge_type, target_type) but positions {np.where(np.logical_not(valid_len_meta_path_parts))[0].tolist()} have an invalid length.\"\n",
    "        assert meta_path[-1][-1] == self.target_node_type, f\"Target type of the last metapath part should be desired target node type but targte node type of the last metapath part is {meta_path[-1][-1]} desired target node type is {self.target_node_type}.\"\n",
    "        \n",
    "        if len(meta_path) <=  1: return\n",
    "        valid_meta_path_parts = []\n",
    "        for meta_path_part_it in range(len(meta_path) - 1):\n",
    "            last_target_type_is_source_type_of_next_edge = meta_path[meta_path_part_it][-1] == meta_path[meta_path_part_it + 1][0]\n",
    "            valid_meta_path_parts.append(last_target_type_is_source_type_of_next_edge)\n",
    "        assert valid_meta_path_parts, \"The target type of each meta path part (triplet) have s´to be the source type of the next meta path part (triplet) in a metapath\"\n",
    "\n",
    "    def add_meta_path(self, hop, meta_path):\n",
    "        \"\"\"\n",
    "        meta_path: List of tuples where each tuple has three parts: source node type, edge type and target node type\n",
    "        \"\"\"\n",
    "        self.assert_meta_path(hop, meta_path)\n",
    "        \n",
    "        if hop not in self.meta_path_dict:\n",
    "            self.meta_path_dict[hop] = []\n",
    "        self.meta_path_dict[hop].append(meta_path)\n",
    "\n",
    "    def add_meta_paths(self, hop, meta_paths):\n",
    "        \"\"\"\n",
    "        meta_paths: List of List of tuples where each tuple has three parts: source node type, edge type and target node type\n",
    "        \"\"\"\n",
    "        for meta_path in meta_paths:\n",
    "            self.add_meta_path(hop, meta_path)\n",
    "\n",
    "    def get_meta_path_dict(self):\n",
    "        return self.meta_path_dict\n",
    "\n",
    "    def get_meta_paths(self):\n",
    "        meta_paths = []\n",
    "        for hop in self.meta_path_dict:\n",
    "            meta_paths.extend(self.meta_path_dict[hop])\n",
    "        return meta_paths\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"\n",
    "        Metapaths: \n",
    "        {self.meta_path_dict}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d39b31-bc23-4f6f-831b-5abd6f0c639a",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "1. Automated construction\n",
    "```\n",
    "meta_paths = MetaPaths(\"author\")\n",
    "depth = 1\n",
    "meta_paths.construct_meta_paths(data.metadata(), depth)\n",
    "```\n",
    "\n",
    "2. Own meta-path defintions\n",
    "```\n",
    "meta_paths = MetaPaths(\"movie\")\n",
    "meta_paths.add_meta_path(1 ,[('actor', 'to', 'movie')])\n",
    "meta_paths.add_meta_path(1 ,[('director', 'to', 'movie')])\n",
    "meta_paths.add_meta_path(2 ,[('movie', 'to', 'director'), ('director', 'to', 'movie')])\n",
    "meta_paths.add_meta_path(2 ,[(\"movie\", \"to\", \"actor\"), (\"actor\", \"to\", \"movie\")])\n",
    "meta_paths.meta_path_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237331d-19c2-48e4-b609-566bbbb36349",
   "metadata": {},
   "source": [
    "## HeteroGraphAware Framework for metapath based aggregation and classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4882b750-274a-437b-ae3f-35f82d56e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class HeteroGraphAware(MetaPaths):\n",
    "    def __init__(self, target_node_type, model, model_hyperparams, add_skip_connection = True, aggregator = \"sum\", add_self_loops = True, include_original_features = True):\n",
    "        ## include_original_features: seems stupid and I would remove it since this can represented by the graph structure (e.g., adding movie -> to -> movie self loops)\n",
    "        super().__init__(target_node_type)\n",
    "        \n",
    "        self.target_node_type = target_node_type # String\n",
    "        self.model = model ## sklearn or xgboost model class!\n",
    "        self.models = []\n",
    "        self.features = []\n",
    "        self.add_skip_connection = add_skip_connection\n",
    "        self.aggregator = aggregator\n",
    "        self.model_hyperparams = model_hyperparams\n",
    "        self.final_model = LogisticRegression(class_weight=\"balanced\")\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.include_original_features = include_original_features\n",
    "        \n",
    "    \n",
    "    def add_self_loops_fun(self, data):\n",
    "        edge_index = torch.cat([torch.arange(data[self.target_node_type].x.shape[0]).unsqueeze(0), torch.arange(data[self.target_node_type].x.shape[0]).unsqueeze(0)], dim = 0)\n",
    "        data[self.target_node_type, \"to\", self.target_node_type].edge_index = edge_index\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def aggregate(source_features, target_features, edge_index, aggregator = \"sum\"):\n",
    "        source_lift = torch.index_select(source_features, 0, edge_index[0])\n",
    "        target_lift = torch.index_select(target_features, 0, edge_index[1])    \n",
    "        input = torch.zeros_like(target_features)\n",
    "        return torch.scatter_reduce(input, 0, edge_index[1].unsqueeze(-1).expand_as(source_lift), source_lift, reduce = aggregator)\n",
    "\n",
    "    def generate_features(self, data, metapath):\n",
    "        source, _, target = metapath[0]\n",
    "        new_features = data[source].x\n",
    "        \n",
    "        for metapath_part in metapath:\n",
    "            source, _, target = metapath_part\n",
    "            new_features = HeteroGraphAware.aggregate(new_features, data[target].x, data[metapath_part].edge_index, self.aggregator)\n",
    "            if self.add_skip_connection:\n",
    "                new_features += data[target].x \n",
    "        return new_features\n",
    "\n",
    "    def include_original_features_fit(self, data, train_mask):                \n",
    "        model_instance = self.model(**self.model_hyperparams)\n",
    "        features = data[self.target_node_type].x[train_mask]\n",
    "        model_instance.fit(features, data[self.target_node_type].y[train_mask])\n",
    "        self.models.append(model_instance)\n",
    "        self.features.append(features)\n",
    "\n",
    "    def include_original_features_predict_proba(self, model, data, test_mask):                \n",
    "        return model.predict_proba(data[self.target_node_type].x)[test_mask]\n",
    "\n",
    "    def fit(self, data, train_mask):\n",
    "        \"\"\"\n",
    "        hyperparams: dict with keys specific for the classifier or regressor model\n",
    "        \"\"\"\n",
    "        if self.add_self_loops:\n",
    "            data = self.add_self_loops_fun(data)\n",
    "        if self.include_original_features:\n",
    "            self.include_original_features_fit(data, train_mask)\n",
    "            \n",
    "        for metapath in self.get_meta_paths():\n",
    "            new_features = self.generate_features(data, metapath)\n",
    "                \n",
    "            model_instance = self.model(**self.model_hyperparams)\n",
    "            model_instance.fit(new_features[train_mask], data[self.target_node_type].y[train_mask])\n",
    "            self.models.append(model_instance)\n",
    "            self.features.append(new_features)\n",
    "        pred_probas = self.get_pred_proba(data, train_mask) # data[\"movie\"].val_mask\n",
    "        \n",
    "        self.final_model.fit(np.concatenate(pred_probas, axis = -1), data[self.target_node_type].y[train_mask])\n",
    "        return self.models\n",
    "\n",
    "    def get_pred_proba(self, data, test_mask):\n",
    "        if self.add_self_loops:\n",
    "            data = self.add_self_loops_fun(data)\n",
    "\n",
    "        pred_probas = []\n",
    "        if self.include_original_features:\n",
    "            model = self.models.pop(0)\n",
    "            pred_probas.append(self.include_original_features_predict_proba(model, data, test_mask))\n",
    "\n",
    "        for i, model in enumerate(self.models):\n",
    "            metapath = self.get_meta_paths()[i]\n",
    "            new_features = self.generate_features(data, metapath)                    \n",
    "            pred_probas.append(model.predict_proba(new_features)[test_mask])\n",
    "        return np.array(pred_probas)\n",
    "        \n",
    "\n",
    "    def predict_proba(self, data, test_mask):\n",
    "        pred_probas = self.get_pred_proba(data, test_mask)\n",
    "        return self.final_model.predict_proba(np.concatenate(pred_probas, axis = -1)) #pred_probas.mean(0) #self.final_model.predict_proba(np.concatenate(pred_probas).transpose())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"\"\"\n",
    "        HeteroGraphAware()\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a828d0-96b9-4ae5-aa17-92d06c561064",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "1) Training\n",
    "```\n",
    "from xgboost import XGBClassifier\n",
    "hetero_graph_aware = HeteroGraphAware(\"author\", XGBClassifier, {\"alpha\": 10}, add_skip_connection=True, include_original_features=False, aggregator=\"sum\")\n",
    "hetero_graph_aware.construct_meta_paths(data.metadata(), 4)\n",
    "hetero_graph_aware.fit(data, data[\"author\"].train_mask)\n",
    "```\n",
    "\n",
    "2) Evaluation\n",
    "```\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(data[\"author\"].y[data[\"author\"].test_mask], hetero_graph_aware.predict_proba(data, data[\"author\"].test_mask).argmax(1), average=\"micro\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbaa37-f225-4a73-a6d9-e7dc32ada088",
   "metadata": {},
   "source": [
    "## Hyperparameter space defintions for different machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3245157-8a3e-44b3-b171-c3d3df6e009b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ModelSpace():\n",
    "    def __init__(self):\n",
    "        self.space = None\n",
    "        self.initialize_space()\n",
    "\n",
    "    def initialize_space(self):\n",
    "        framework_choices = {\n",
    "            'hops': [3], #2, 3, 4\n",
    "            'aggregator': [\"sum\"],#, \"mean\"\n",
    "            'add_skip_connection': [True],#False\n",
    "        }\n",
    "         \n",
    "        self.space = {\n",
    "            **{key: hp.choice(key, value) for key, value in framework_choices.items()}\n",
    "        }\n",
    "        \n",
    "    def add_choice(self, key, items):\n",
    "        self.space[key] = hp.choice(key, items)\n",
    "        \n",
    "    def add_uniform(self, key, limits: tuple):\n",
    "        self.space[key] = hp.uniform(key, limits[0], limits[1])\n",
    "        \n",
    "    def add_loguniform(self, key, limits: tuple):\n",
    "        self.space[key] = hp.loguniform(key, np.log(limits[0]), np.log(limits[1]))\n",
    "        \n",
    "    def add_qloguniform(self, key, limits, q):\n",
    "        self.space[key] = hp.qloguniform(key, low=np.log(limits[0]), high=np.log(limits[1]), q=q)\n",
    "\n",
    "class LogitsticRegressionSpace(ModelSpace):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_space(self):\n",
    "        self.add_choice(\"n_jobs\", [-1])\n",
    "        # self.add_loguniform('tol', [6e-3, 4e-2])\n",
    "        self.add_uniform('C', [0, 10])\n",
    "        return self.space    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269d122-4c0a-4958-b46f-3739baefdfec",
   "metadata": {},
   "source": [
    "## Evaluation on nested Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08ec1a58-d0ba-4cc2-a9f1-e70218faa717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_masks(train_mask, manual_seed = None, train_size = 0.8):\n",
    "    if manual_seed:\n",
    "        torch.manual_seed(manual_seed)\n",
    "    train_index = train_mask.nonzero().squeeze()\n",
    "    min = int(train_size*train_index.shape[0])\n",
    "    rand_train_index = torch.randperm(train_index.shape[0])\n",
    "    rand_train_index_train_index = rand_train_index[:min]\n",
    "    rand_train_index_val_index = rand_train_index[min:]\n",
    "\n",
    "    train_mask = torch.zeros_like(train_mask)\n",
    "    val_mask = torch.zeros_like(train_mask)\n",
    "    \n",
    "    new_train_idx = train_index[rand_train_index_train_index]\n",
    "    new_val_idx = train_index[rand_train_index_val_index]\n",
    "\n",
    "    train_mask[new_train_idx] = 1\n",
    "    val_mask[new_val_idx] = 1\n",
    "    return train_mask, val_mask\n",
    "\n",
    "def space_to_spaces(space, hops):\n",
    "    spaces = []\n",
    "    for hop in hops:\n",
    "        spaces.append(copy.deepcopy(space))\n",
    "    return spaces\n",
    "\n",
    "class HeteroGraphAwareNestedCVEvaluation:\n",
    "\n",
    "    def __init__(self,target_node_type, device_id, model, data, minimize = True, max_evals = 100, parallelism = 1):\n",
    "        self.target_node_type = target_node_type\n",
    "        self.device_id = device_id\n",
    "        self.model = model\n",
    "        self.training_times = []\n",
    "        self.minimize = minimize\n",
    "        self.data = data\n",
    "        self.nested_transd_cv = None\n",
    "        self.max_evals = max_evals\n",
    "        self.parallelism = parallelism\n",
    "\n",
    "    def nested_cross_validate(self, k_outer, k_inner, space):  \n",
    "\n",
    "        # spaces = space_to_spaces()        \n",
    "        def evaluate_fun(fitted_model, data, mask):\n",
    "            pred_proba = fitted_model.predict_proba(data, mask)\n",
    "            return f1_score(data[self.target_node_type].y[mask], pred_proba.argmax(1), average=\"micro\")\n",
    "\n",
    "        def train_fun(data, inner_train_mask, hyperparameters):  \n",
    "            hops = hyperparameters[\"hops\"]\n",
    "            aggregator = hyperparameters[\"aggregator\"]\n",
    "            add_skip_connection = hyperparameters[\"add_skip_connection\"]\n",
    "\n",
    "            filtered_keys = list(filter(lambda key: key not in [\"aggregator\", \"hops\", \"add_skip_connection\"], hyperparameters.keys()))\n",
    "            model_hyperparams = {key: hyperparameters[key] for key in filtered_keys}\n",
    "            \n",
    "            hetero_graph_aware = HeteroGraphAware(self.target_node_type, self.model, model_hyperparams, add_skip_connection=add_skip_connection, include_original_features=False, aggregator=aggregator)\n",
    "            ## TODO test all bool combination\n",
    "            hetero_graph_aware.construct_meta_paths(data.metadata(), hops)\n",
    "            start_time = time.time()\n",
    "            hetero_graph_aware.fit(data, inner_train_mask)\n",
    "            end_time = time.time() - start_time\n",
    "            print(end_time)\n",
    "            return hetero_graph_aware\n",
    "            \n",
    "        self.nested_transd_cv = NestedTransductiveCV(self.data, self.target_node_type, k_outer, k_inner, train_fun, evaluate_fun,max_evals = self.max_evals, parallelism = self.parallelism, minimalize = self.minimize)\n",
    "        self.nested_transd_cv.outer_cv(space)\n",
    "        return self.nested_transd_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2231441f-06f0-4df0-8a90-79ef17ea3e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "        Using a 3 x 3 nested StratifiedKFold Cross-Validation, we obtain:\n",
       "        0.8802 +- 0.0166.\n",
       "\n",
       "        self.outer_scores: [0.86252772 0.87573964 0.90236686]\n",
       "\n",
       "        self.best_params_per_fold: [{'C': 2.115256507027895, 'add_skip_connection': True, 'aggregator': 'sum', 'hops': 3, 'n_jobs': -1}, {'C': 1.8462062927881318, 'add_skip_connection': True, 'aggregator': 'sum', 'hops': 3, 'n_jobs': -1}, {'C': 0.13506876976748128, 'add_skip_connection': True, 'aggregator': 'sum', 'hops': 3, 'n_jobs': -1}]\n",
       "\n",
       "        self.best_models: []\n",
       "\n",
       "        "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hetero_graph_aware_evaluation = HeteroGraphAwareNestedCVEvaluation(\"author\", 2, LogisticRegression, data, minimize = False, max_evals = 3, parallelism = 1)\n",
    "space = LogitsticRegressionSpace().get_space()\n",
    "hetero_graph_aware_evaluation.nested_cross_validate(3,3, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6437a5-0852-4bc5-98ca-05327bf74251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
